{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "from notify_run import Notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify = Notify()\n",
    "notify.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('/users/dli44/tool-presence'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src import constants as c\n",
    "# from src.model import VAE\n",
    "from src.model import loss_function\n",
    "from src import visualization as v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(c.image_size),\n",
    "    transforms.CenterCrop(c.image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(c.data_home, 'surgical_data', x),\n",
    "                                          data_transforms)\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                              batch_size=c.batch_size,\n",
    "                                              shuffle=True)\n",
    "               for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing weaker models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, h_dim1, h_dim2, zdim, conv_channels=[16, 16]):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.h_dim1 = h_dim1\n",
    "        self.h_dim2 = h_dim2\n",
    "        self.zdim = zdim\n",
    "        self.conv_channels = conv_channels\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(image_channels, conv_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(conv_channels[0], conv_channels[1], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Latent vectors\n",
    "        self.fc1 = nn.Linear(image_size//2 * image_size//2 * conv_channels[1], h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "#         self.fc1 = nn.Linear(image_size//2 * image_size//2 * conv_channels[1], h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, zdim)\n",
    "        self.fc32 = nn.Linear(h_dim2, zdim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(zdim, h_dim2)\n",
    "#         self.fc5 = nn.Linear(h_dim2, image_size//2 * image_size//2 * conv_channels[1])\n",
    "        self.fc4 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc5 = nn.Linear(h_dim1, image_size//2 * image_size//2 * conv_channels[1])\n",
    "        \n",
    "        self.conv3 = nn.ConvTranspose2d(conv_channels[1], conv_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv4 = nn.ConvTranspose2d(conv_channels[0], image_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.dropout(self.pool1(x))\n",
    "        x = x.view(-1, self.image_size//2 * self.image_size//2 * self.conv_channels[-1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc31(x), self.fc32(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = F.relu(self.fc3(z))\n",
    "        z = F.relu(self.fc4(z))\n",
    "        z = F.relu(self.fc5(z))\n",
    "        z = z.view(-1, self.conv_channels[-1], self.image_size//2, self.image_size//2)\n",
    "        z = F.interpolate(z, scale_factor=2)       \n",
    "        z = F.relu(self.conv3(z))\n",
    "        z = torch.sigmoid(self.conv4(z))\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta in [1, 5, 50, 100]:\n",
    "    output_name = 'beta_{}_2fc_vae_{}_epoch_{}_zdim_{}.{}'\n",
    "    losses = {'kl':[], 'rl':[]}\n",
    "    \n",
    "    zdim = 64\n",
    "    model = VAE(image_channels=c.image_channels,\n",
    "                image_size=c.image_size, \n",
    "                h_dim1=1024,\n",
    "                h_dim2=128,\n",
    "                zdim=zdim).to(c.device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    notify.send(\"Starting training for zdim {}\".format(zdim))\n",
    "    loss_params = {'input_size': c.image_size,\n",
    "                   'zdim': zdim,\n",
    "                   'beta': beta\n",
    "                  }\n",
    "    \n",
    "    t1 = tnrange(50)\n",
    "    for epoch in t1:\n",
    "        model.train()\n",
    "        train_loss, kl, rl = 0, 0, 0\n",
    "        t2 = tqdm_notebook(dataloaders['train'])\n",
    "        for batch_idx, (data, _) in enumerate(t2):\n",
    "            data = data.to(c.device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss, r, k = loss_function(recon_batch, data, mu, logvar, **loss_params)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            kl += k.item()\n",
    "            rl += r.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            t2.set_postfix({\"Reconstruction Loss\":r.item(), \"KL Divergence\":k.item()})\n",
    "\n",
    "        losses['kl'].append(kl)\n",
    "        losses['rl'].append(rl)\n",
    "        notify.send(\"z-dim = {}, Training Epoch {}, Training Loss: {:.4f}\".format(zdim, \n",
    "                                                                                  epoch+1,\n",
    "                                                                                  train_loss / len(dataloaders['train'].dataset)))\n",
    "\n",
    "        t1.set_postfix({\"KL Divergence\":kl/len(dataloaders['train'].dataset), \n",
    "                       \"Reconstruction Loss\":rl/len(dataloaders['train'].dataset)})\n",
    "        \n",
    "        \"\"\"\n",
    "        Testing\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (data, _) in enumerate(dataloaders['val']):\n",
    "                data = data.to(c.device)\n",
    "                recon_batch, mu, logvar = model(data)\n",
    "                loss, r, k = loss_function(recon_batch, data, mu, logvar, **loss_params)\n",
    "                test_loss += loss.item()\n",
    "                if i == 0:\n",
    "                    n = min(data.size(0), 8)\n",
    "                    comparison = torch.cat([data[:n],\n",
    "                                            recon_batch.view(c.batch_size, \n",
    "                                                             c.image_channels, \n",
    "                                                             c.image_size, \n",
    "                                                             c.image_size)[:n]])\n",
    "\n",
    "                    save_image(comparison.cpu(),\n",
    "                               os.path.join(data_home,\n",
    "                                            'samples',\n",
    "                                            output_name.format(beta, c.image_size, epoch+1, zdim, 'png')\n",
    "                                           ), nrow=n)\n",
    "                    \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), \n",
    "                       os.path.join(data_home, 'weights',\n",
    "                                    output_name.format(beta, c.image_size, epoch+1, zdim, 'torch')))\n",
    "            notify.send(\"Saved weights at epoch {}\".format(epoch+1))\n",
    "                        \n",
    "    fig = plt.figure()\n",
    "    plt.plot(losses['kl'])\n",
    "    plt.plot(losses['rl'])\n",
    "    plt.savefig(os.path.join(data_home, 'figures', output_name.format(beta, c.image_size, '50', zdim, 'png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"Initial Images\\nStart, End\")\n",
    "plt.imshow(np.hstack([image_datasets['train'][0][0].numpy().transpose(1,2,0), \n",
    "                      image_datasets['train'][10][0].numpy().transpose(1,2,0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = v.latent_interpolation(image_datasets['train'][0][0], image_datasets['train'][10][0], model=model)\n",
    "\n",
    "fig, ax = plt.subplots(1,10, figsize=(10,2),\n",
    "                       frameon=False,gridspec_kw={'wspace':0.05, 'width_ratios':[1.25,1,1,1,1,1,1,1,1,1.25]})\n",
    "for i in range(10):\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "ax[0].set_title(\"Start\")\n",
    "ax[-1].set_title(\"End\")\n",
    "\n",
    "# plt.savefig(os.path.join(data_home,'figures','tool_different_anatomy_similar.png'), bbox_inches='tight', dpi=400, pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"Initial Images\\nStart, End\")\n",
    "plt.imshow(np.hstack([image_datasets['train'][10][0].numpy().transpose(1,2,0), \n",
    "                      image_datasets['train'][830][0].numpy().transpose(1,2,0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = v.latent_interpolation(image_datasets['train'][10][0], image_datasets['train'][830][0], model=model)\n",
    "\n",
    "fig, ax = plt.subplots(1,10, figsize=(10,2),\n",
    "                       frameon=False,gridspec_kw={'wspace':0.05, 'width_ratios':[1.25,1,1,1,1,1,1,1,1,1.25]})\n",
    "for i in range(10):\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')\n",
    "ax[0].set_title(\"Start\")\n",
    "ax[-1].set_title(\"End\")\n",
    "# plt.savefig(os.path.join(data_home, 'figures', 'tool_different_anatomy_different.png'),bbox_inches='tight', dpi=400, pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
